# Azure IaC Benchmark ğŸ

A fair, reproducible benchmark comparing deployment speeds of **Bicep**, **Terraform**, **OpenTofu**, and **Pulumi** on Azure.

![Azure](https://img.shields.io/badge/Azure-0078D4?style=flat&logo=microsoftazure&logoColor=white)
![Bicep](https://img.shields.io/badge/Bicep-0078D4?style=flat&logo=microsoftazure&logoColor=white)
![Terraform](https://img.shields.io/badge/Terraform-7B42BC?style=flat&logo=terraform&logoColor=white)
![OpenTofu](https://img.shields.io/badge/OpenTofu-FFDA18?style=flat&logo=opentofu&logoColor=black)
![Pulumi](https://img.shields.io/badge/Pulumi-8A3391?style=flat&logo=pulumi&logoColor=white)

## ğŸ“Š Benchmark Results

Results from 3 iterations on identical Azure resources (January 2026):

<p align="center">
  <img src="assets/benchmark-report.png" alt="Azure IaC Benchmark Report" width="900">
</p>

### Deployment Times

| Tool | Min | Max | Average |
|------|-----|-----|---------|
| ğŸ¥‡ Pulumi | 23.13s | 24.23s | **23.64s** |
| ğŸ¥ˆ Bicep | 34.73s | 35.62s | **35.22s** |
| ğŸ¥‰ Terraform | 42.15s | 46.81s | **44.15s** |
| OpenTofu | 44.12s | 45.84s | **44.84s** |

### Destroy Times

| Tool | Min | Max | Average |
|------|-----|-----|---------|
| ğŸ¥‡ Pulumi | 13.55s | 15.17s | **14.12s** |
| ğŸ¥ˆ Bicep | 16.82s | 29.54s | **21.39s** |
| ğŸ¥‰ Terraform | 60.32s | 61.87s | **60.93s** |
| OpenTofu | 59.42s | 63.23s | **61.15s** |

### ğŸ† Winners

- **Fastest Deploy**: Pulumi (23.64s avg) - 49% faster than Bicep, 87% faster than Terraform/OpenTofu
- **Fastest Destroy**: Pulumi (14.12s avg) - 51% faster than Bicep, 332% faster than Terraform/OpenTofu
- **Most Consistent**: Pulumi (smallest variance across iterations)
- **Note**: OpenTofu and Terraform have nearly identical performance (expected as OpenTofu is a Terraform fork)

## ğŸ—ï¸ Resources Deployed

Each tool deploys identical infrastructure:

- **Virtual Network** with 3 subnets (10.0.0.0/16)
  - Default subnet (10.0.0.0/24)
  - Private subnet (10.0.1.0/24)
  - Private subnet 2 (10.0.2.0/24)
- **Network Security Group** with 3 rules (HTTP, HTTPS, SSH)
- **Storage Account** (Standard_LRS, StorageV2)
- **App Service Plan** (Linux, B1 SKU)
- **Log Analytics Workspace** (30-day retention)

## ğŸš€ Quick Start

### Prerequisites

| Tool | Installation |
|------|--------------|
| Azure CLI | [Install Guide](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli) |
| Bicep CLI | Included with Azure CLI 2.20.0+ |
| Terraform | [Download](https://www.terraform.io/downloads) (v1.0+) |
| OpenTofu | [Install Guide](https://opentofu.org/docs/intro/install/) (v1.6+) |
| Pulumi | [Install Guide](https://www.pulumi.com/docs/get-started/install/) (v3.0+) |
| Python | 3.8+ (for Pulumi) |

### One-Command Setup

```bash
# Clone the repo
git clone https://github.com/mwhooo/azure-iac-benchmark.git
cd azure-iac-benchmark

# Run setup (creates resource groups, initializes all tools)
./setup.sh
```

The setup script will:
1. âœ… Check all prerequisites are installed
2. âœ… Verify Azure CLI login
3. âœ… Create 4 resource groups (one per tool)
4. âœ… Initialize Terraform with providers
5. âœ… Initialize OpenTofu with providers
6. âœ… Create Pulumi virtual environment and stack
7. âœ… Verify Bicep templates compile

### Run Benchmark

```bash
# Run 3 iterations (default)
./run-iterations.sh

# Run 5 iterations for more statistical significance
./run-iterations.sh 5
```

Results are saved to `results/` directory:
- **JSON**: `benchmark_TIMESTAMP.json` - Raw data for programmatic access
- **HTML**: `benchmark_TIMESTAMP.html` - Interactive report with charts (auto-opens in browser)

## ğŸ“ Project Structure

```
azure-iac-benchmark/
â”œâ”€â”€ bicep/                    # Bicep templates
â”‚   â”œâ”€â”€ main-template.bicep
â”‚   â”œâ”€â”€ main-template.bicepparam
â”‚   â””â”€â”€ bicep-modules/        # Reusable modules
â”œâ”€â”€ terraform/                # Terraform configuration
â”‚   â”œâ”€â”€ main.tf
â”‚   â”œâ”€â”€ variables.tf
â”‚   â”œâ”€â”€ outputs.tf
â”‚   â””â”€â”€ providers.tf
â”œâ”€â”€ opentofu/                 # OpenTofu configuration (Terraform-compatible)
â”‚   â”œâ”€â”€ main.tf
â”‚   â”œâ”€â”€ variables.tf
â”‚   â”œâ”€â”€ outputs.tf
â”‚   â””â”€â”€ providers.tf
â”œâ”€â”€ pulumi/                   # Pulumi Python project
â”‚   â”œâ”€â”€ __main__.py
â”‚   â”œâ”€â”€ Pulumi.yaml
â”‚   â”œâ”€â”€ Pulumi.benchmark.yaml # Stack configuration
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ results/                  # Benchmark results (gitignored)
â”‚   â”œâ”€â”€ benchmark_*.json      # Raw timing data
â”‚   â””â”€â”€ benchmark_*.html      # Interactive HTML reports
â”œâ”€â”€ setup.sh                  # One-command setup
â”œâ”€â”€ run-iterations.sh         # Multi-iteration benchmark
â””â”€â”€ run-benchmark.sh          # Single benchmark with options
```

## ğŸ“Š HTML Reports

After running the benchmark, an interactive HTML report is automatically generated and opened in your browser.

**Features:**
- ğŸ† **Winner banner** with overall champion
- ğŸ“Š **Tool cards** with deploy/destroy times and ranges  
- ğŸ“ˆ **Bar charts** comparing average deploy and destroy times
- ğŸ“‰ **Line chart** showing performance consistency per iteration
- âš–ï¸ **Comparison table** with percentage differences vs winner

Reports are saved to `results/benchmark_TIMESTAMP.html` and can be shared or archived.

## âš™ï¸ Configuration

### Custom Location

```bash
# Set location before running setup
export LOCATION=eastus
./setup.sh
```

### Custom Resource Groups

```bash
export BICEP_RG=my-bicep-rg
export TERRAFORM_RG=my-terraform-rg
export OPENTOFU_RG=my-opentofu-rg
export PULUMI_RG=my-pulumi-rg
./setup.sh
```

## ğŸ” Why These Results?

### Pulumi's Speed Advantage

1. **Azure Native Provider** - Direct ARM API calls without translation layers
2. **Parallel Execution** - Deploys independent resources simultaneously
3. **Efficient State Management** - Optimized diffing algorithms
4. **Native Destroy** - Built-in resource deletion vs manual cleanup

### Terraform's Consistency

- Most consistent deploy times (lowest variance)
- Mature provider with predictable behavior
- Slower destroy due to strict dependency ordering

### Bicep's Variability

- Native Azure tool, no additional runtime
- Higher variance due to ARM deployment engine
- No native destroy command (requires manual resource deletion)

### OpenTofu Comparison

- Open-source fork of Terraform (MPL 2.0 license)
- Uses same HCL configuration language
- Compatible with Terraform providers
- Community-driven development
- Performance should be very similar to Terraform

## ğŸ“ˆ Methodology

- All tools deploy to **separate resource groups** in the same region
- Timing measured with `date +%s.%N` (nanosecond precision)
- Each iteration: **full deploy â†’ verify â†’ full destroy**
- Output suppressed to eliminate I/O timing differences
- Same Azure subscription and network conditions
- Resources verified identical across all four tools

## ğŸ§¹ Cleanup

Remove all benchmark resources:

```bash
az group delete -n azure-iac-benchmark-bicep-rg --yes --no-wait
az group delete -n azure-iac-benchmark-terraform-rg --yes --no-wait
az group delete -n azure-iac-benchmark-opentofu-rg --yes --no-wait
az group delete -n azure-iac-benchmark-pulumi-rg --yes --no-wait
```

## ğŸ¤ Contributing

Contributions welcome! Ideas for improvement:

- [x] ~~Add OpenTofu to the comparison~~
- [ ] Add AWS CDK / CloudFormation comparison
- [ ] GitHub Actions workflow for automated benchmarks
- [ ] More complex infrastructure scenarios
- [ ] Cost comparison alongside speed
- [ ] Memory/CPU usage during deployments

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) for details.

---

*Benchmark conducted on Azure West Europe region. Your results may vary based on region, subscription type, and network conditions.*
